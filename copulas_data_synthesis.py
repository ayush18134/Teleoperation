# -*- coding: utf-8 -*-
"""copulas_data_synthesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UHs2Hv-C4VhVH_9cWBlZS9sJNG92df8U

## Install Dependencies
"""

!pip install numpy
!pip install copulas
!pip install table-evaluator

"""##Dataset Class"""

#Dataset format - Pandas Dataframe
#Rows are the participants
#Columns are time to completion, number of collisions, number of dropped objects and Spatial ability scores

import pandas as pd
import numpy as np
temp=[233,190,286,445,281,260,301,322,245,418,164,278,309,309]
class Dataset():
    def getData(self):
        dataset = pd.DataFrame(np.array([[ 233, 2.0, 0.8, 68.0],
                                         [ 190, 0.2, 0.0, 70.4],
                                         [ 286, 0.4, 0.0, 58.4],
                                         [ 445, 4.6, 3.0, 52.0],
                                         [ 281, 1.8, 0.6, 65.6],
                                         [ 260, 2.3, 0.8, 80.0],
                                         [ 301, 0.6, 1.0, 64.0],
                                         [ 322, 2.8, 1.2, 52.0],
                                         [ 245, 2.0, 0.0, 88.0],
                                         [ 418, 2.3, 0.7, 57.0],
                                         [ 164, 0.6, 0.2, 65.0],
                                         [ 278, 0.6, 0.0, 87.0],
                                         [ 309, 1.2, 0.8, 53.0],
                                         [ 309, 0.0, 0.3, 84.0]]), columns = [ 'Total Time', 'Number of collisions', 'Number of Dropped Objects', 'Spatial Ability'])
        self.dataset = dataset
        return dataset.iloc[:, 1:]

    def getScaledData(self):
        dataset = self.getDataset()
        for col in dataset:
            dataset[col] = dataset[col] / np.max(dataset[col])
        return dataset

"""## Synthesize Data"""

from copulas.multivariate import GaussianMultivariate
from copulas.visualization import compare_3d

dataset = Dataset()
real_data = dataset.getData()


# Initiates the Copulas and call its fit method to pass in the table
copula = GaussianMultivariate()
# copula = VineCopula('regular') # center, regular, direct
copula.fit(real_data)

print(real_data.to_string())

# Generate synthetic data
synthetic_data = copula.sample(10000)
print(synthetic_data.to_string())

# Plot the real and the synthetic data to compare
compare_3d(real_data, synthetic_data)

"""## Verify Obtained data"""

from table_evaluator import TableEvaluator
table_evaluator = TableEvaluator(real_data, synthetic_data)
table_evaluator.visual_evaluation()

"""## Data Preparation"""

import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# synthetic_data=synthetic_data[(synthetic_data> 0).all(1)].reset_index(drop=True)
synthetic_data_new=np.array(synthetic_data)
real_data = np.array(real_data)

trainX=[]
trainY=[]
testX=[]
testY=[]

# Combination 1 - Combine synthetic and real data to form test and train datasets
# # Adding 4 rows of real data in training set
for i in range(4):
    testX.append([real_data[i][0],real_data[i][1]])
    testY.append(real_data[i][2])
# Adding 10 rows of real data in testing set
for i in range(4,len(real_data)):
    trainX.append([real_data[i][0],real_data[i][1]])
    trainY.append(real_data[i][2])
# Adding 80% rows of synthetic data in training set
for i in range(int(len(synthetic_data_new)/5)):
    testX.append([synthetic_data_new[i][0],synthetic_data_new[i][1]])
    testY.append(synthetic_data_new[i][2])
# Adding 20% rows of synthetic data in training set
for i in range(int(len(synthetic_data_new)/5),len(synthetic_data_new)):
    trainX.append([synthetic_data_new[i][0],synthetic_data_new[i][1]])
    trainY.append(synthetic_data_new[i][2])

#Combination 2 - Use synthetic as train and real as test
# trainX = synthetic_data.iloc[:, :-1]
# trainY = synthetic_data.iloc[:, -1]

# testX = real_data.iloc[:, :-1]
# testY = real_data.iloc[:, -1]



from numpy.random import random
Spatial_ability=[]
num_of_objects=[]
num_of_collision=[]
for i in range(len(trainX)):
    Spatial_ability.append(trainY[i])
    num_of_objects.append(trainX[i][0])
    num_of_collision.append(trainX[i][1])

pip install pyCompare

import pyCompare
pyCompare.blandAltman(Spatial_ability, num_of_objects)

pyCompare.blandAltman(Spatial_ability, num_of_collision)

"""## Models

### Elastic Net
"""

from sklearn.linear_model import ElasticNet
import numpy as np
model = ElasticNet()
model.fit(trainX, trainY)
print(model.score(trainX, trainY))
print(model.score(testX, testY))

predictedData = model.predict(trainX)
# print(predictedData)
predictedData = model.predict(testX)
# print(predictedData)

"""### Linear Regression"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

regr = linear_model.LinearRegression()

regr.fit(trainX, trainY)
print("train")
predictedData = regr.predict(trainX)
count=[0]*10;
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-trainY[i])/trainY[i])*100
    if(change<5):
        count[0]+=1*(100/len(trainX))
    elif(change<10):
        count[1]+=1*(100/len(trainX))
    elif(change<15):
        count[2]+=1*(100/len(trainX))
    elif(change<20):
        count[3]+=1*(100/len(trainX))
    elif(change<25):
        count[4]+=1*(100/len(trainX))
    elif(change<30):
        count[5]+=1*(100/len(trainX))
    elif(change<35):
        count[6]+=1*(100/len(trainX))
    elif(change<40):
        count[7]+=1*(100/len(trainX))
    elif(change<45):
        count[8]+=1*(100/len(trainX))
    else:
        count[9]+=1*(100/len(trainX))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(trainY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(trainY, predictedData))
print("test")
predictedData = regr.predict(testX)
count=[0]*10
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-testY[i])/testY[i])*100
    if(change<5):
        count[0]+=1*(100/len(testY))
    elif(change<10):
        count[1]+=1*(100/len(testY))
    elif(change<15):
        count[2]+=1*(100/len(testY))
    elif(change<20):
        count[3]+=1*(100/len(testY))
    elif(change<25):
        count[4]+=1*(100/len(testY))
    elif(change<30):
        count[5]+=1*(100/len(testY))
    elif(change<35):
        count[6]+=1*(100/len(testY))
    elif(change<40):
        count[7]+=1*(100/len(testY))
    elif(change<45):
        count[8]+=1*(100/len(testY))
    else:
        count[9]+=1*(100/len(testY))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(testY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(testY, predictedData))

pyCompare.blandAltman(testY, predictedData)

"""### SVM Regression"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))
regr.fit(trainX, trainY)
print("train")
predictedData = regr.predict(trainX)
count=[0]*10;
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-trainY[i])/trainY[i])*100
    if(change<5):
        count[0]+=1*(100/len(trainX))
    elif(change<10):
        count[1]+=1*(100/len(trainX))
    elif(change<15):
        count[2]+=1*(100/len(trainX))
    elif(change<20):
        count[3]+=1*(100/len(trainX))
    elif(change<25):
        count[4]+=1*(100/len(trainX))
    elif(change<30):
        count[5]+=1*(100/len(trainX))
    elif(change<35):
        count[6]+=1*(100/len(trainX))
    elif(change<40):
        count[7]+=1*(100/len(trainX))
    elif(change<45):
        count[8]+=1*(100/len(trainX))
    else:
        count[9]+=1*(100/len(trainX))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(trainY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(trainY, predictedData))
print("test")
predictedData = regr.predict(testX)
count=[0]*10
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-testY[i])/testY[i])*100
    if(change<5):
        count[0]+=1*(100/len(testY))
    elif(change<10):
        count[1]+=1*(100/len(testY))
    elif(change<15):
        count[2]+=1*(100/len(testY))
    elif(change<20):
        count[3]+=1*(100/len(testY))
    elif(change<25):
        count[4]+=1*(100/len(testY))
    elif(change<30):
        count[5]+=1*(100/len(testY))
    elif(change<35):
        count[6]+=1*(100/len(testY))
    elif(change<40):
        count[7]+=1*(100/len(testY))
    elif(change<45):
        count[8]+=1*(100/len(testY))
    else:
        count[9]+=1*(100/len(testY))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(testY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(testY, predictedData))
pyCompare.blandAltman(testY, predictedData)

N = len(trainX)
p = len(trainX[0]) + 1

X_with_intercept = np.empty(shape=(N, p), dtype=np.float)
X_with_intercept[:, 0] = 1
X_with_intercept[:, 1:p] = trainX

y_hat = regr.predict(trainX)
residuals = trainY - y_hat
residual_sum_of_squares = residuals.T @ residuals
sigma_squared_hat = residual_sum_of_squares / (N - p)
var_beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) * sigma_squared_hat
for p_ in range(p):
    standard_error = var_beta_hat[p_, p_] ** 0.5
    print(f"SE(beta_hat[{p_}]): {standard_error}")

"""### KNN"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neighbors import KNeighborsRegressor
regr = KNeighborsRegressor(n_neighbors=2)
regr.fit(trainX, trainY)
print("train")
predictedData = regr.predict(trainX)
count=[0]*10;
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-trainY[i])/trainY[i])*100
    if(change<5):
        count[0]+=1*(100/len(trainX))
    elif(change<10):
        count[1]+=1*(100/len(trainX))
    elif(change<15):
        count[2]+=1*(100/len(trainX))
    elif(change<20):
        count[3]+=1*(100/len(trainX))
    elif(change<25):
        count[4]+=1*(100/len(trainX))
    elif(change<30):
        count[5]+=1*(100/len(trainX))
    elif(change<35):
        count[6]+=1*(100/len(trainX))
    elif(change<40):
        count[7]+=1*(100/len(trainX))
    elif(change<45):
        count[8]+=1*(100/len(trainX))
    else:
        count[9]+=1*(100/len(trainX))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(trainY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(trainY, predictedData))
print("test")
predictedData = regr.predict(testX)
count=[0]*10
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-testY[i])/testY[i])*100
    if(change<5):
        count[0]+=1*(100/len(testY))
    elif(change<10):
        count[1]+=1*(100/len(testY))
    elif(change<15):
        count[2]+=1*(100/len(testY))
    elif(change<20):
        count[3]+=1*(100/len(testY))
    elif(change<25):
        count[4]+=1*(100/len(testY))
    elif(change<30):
        count[5]+=1*(100/len(testY))
    elif(change<35):
        count[6]+=1*(100/len(testY))
    elif(change<40):
        count[7]+=1*(100/len(testY))
    elif(change<45):
        count[8]+=1*(100/len(testY))
    else:
        count[9]+=1*(100/len(testY))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(testY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(testY, predictedData))
pyCompare.blandAltman(testY, predictedData)

"""### Random Forest"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

regr = RandomForestRegressor(random_state=0, max_depth=None, )
regr.fit(trainX, trainY)
print("train")
predictedData = regr.predict(trainX)
count=[0]*10;
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-trainY[i])/trainY[i])*100
    if(change<5):
        count[0]+=1*(100/len(trainX))
    elif(change<10):
        count[1]+=1*(100/len(trainX))
    elif(change<15):
        count[2]+=1*(100/len(trainX))
    elif(change<20):
        count[3]+=1*(100/len(trainX))
    elif(change<25):
        count[4]+=1*(100/len(trainX))
    elif(change<30):
        count[5]+=1*(100/len(trainX))
    elif(change<35):
        count[6]+=1*(100/len(trainX))
    elif(change<40):
        count[7]+=1*(100/len(trainX))
    elif(change<45):
        count[8]+=1*(100/len(trainX))
    else:
        count[9]+=1*(100/len(trainX))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(trainY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(trainY, predictedData))
print("test")
predictedData = regr.predict(testX)
count=[0]*10
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-testY[i])/testY[i])*100
    if(change<5):
        count[0]+=1*(100/len(testY))
    elif(change<10):
        count[1]+=1*(100/len(testY))
    elif(change<15):
        count[2]+=1*(100/len(testY))
    elif(change<20):
        count[3]+=1*(100/len(testY))
    elif(change<25):
        count[4]+=1*(100/len(testY))
    elif(change<30):
        count[5]+=1*(100/len(testY))
    elif(change<35):
        count[6]+=1*(100/len(testY))
    elif(change<40):
        count[7]+=1*(100/len(testY))
    elif(change<45):
        count[8]+=1*(100/len(testY))
    else:
        count[9]+=1*(100/len(testY))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(testY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(testY, predictedData))
pyCompare.blandAltman(testY, predictedData)

"""### MLP"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
regr =MLPRegressor(random_state=1, max_iter=500)
regr.fit(trainX, trainY)
print("train")
predictedData = regr.predict(trainX)
count=[0]*10;
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-trainY[i])/trainY[i])*100
    if(change<5):
        count[0]+=1*(100/len(trainX))
    elif(change<10):
        count[1]+=1*(100/len(trainX))
    elif(change<15):
        count[2]+=1*(100/len(trainX))
    elif(change<20):
        count[3]+=1*(100/len(trainX))
    elif(change<25):
        count[4]+=1*(100/len(trainX))
    elif(change<30):
        count[5]+=1*(100/len(trainX))
    elif(change<35):
        count[6]+=1*(100/len(trainX))
    elif(change<40):
        count[7]+=1*(100/len(trainX))
    elif(change<45):
        count[8]+=1*(100/len(trainX))
    else:
        count[9]+=1*(100/len(trainX))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(trainY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(trainY, predictedData))
print("test")
predictedData = regr.predict(testX)
count=[0]*10
for i in range(len(predictedData)):
    change=(abs(predictedData[i]-testY[i])/testY[i])*100
    if(change<5):
        count[0]+=1*(100/len(testY))
    elif(change<10):
        count[1]+=1*(100/len(testY))
    elif(change<15):
        count[2]+=1*(100/len(testY))
    elif(change<20):
        count[3]+=1*(100/len(testY))
    elif(change<25):
        count[4]+=1*(100/len(testY))
    elif(change<30):
        count[5]+=1*(100/len(testY))
    elif(change<35):
        count[6]+=1*(100/len(testY))
    elif(change<40):
        count[7]+=1*(100/len(testY))
    elif(change<45):
        count[8]+=1*(100/len(testY))
    else:
        count[9]+=1*(100/len(testY))
for i in range(1,10):
    count[i]=count[i]+count[i-1]
print(count)
print('Mean squared error: %.2f'
#       % mean_squared_error(testY, predictedData))
print('Coefficient of determination: %.2f'
#       % r2_score(testY, predictedData))

pyCompare.blandAltman(testY, predictedData)